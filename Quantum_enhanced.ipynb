{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nithin2211/Quantam-Enhanced/blob/main/Quantum_enhanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "NtaI_-HyFIz-",
        "outputId": "fa6ab89d-4ab0-4b06-bf91-dab5f1f2f25c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4d0ba449-8bb3-4b37-99a1-2d68f8caea0b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4d0ba449-8bb3-4b37-99a1-2d68f8caea0b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-814238468.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Upload files to Colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Select the 3 CSV files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# This will upload:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload files to Colab\n",
        "uploaded = files.upload()  # Select the 3 CSV files\n",
        "\n",
        "# This will upload:\n",
        "# - fer2013_sample.csv\n",
        "# - pie_trajectory_sample.csv\n",
        "# - combined_emotion_trajectory_sample.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load FER-2013 dataset\n",
        "fer2013_df = pd.read_csv('fer2013_sample.csv')\n",
        "print(fer2013_df.head())\n",
        "\n",
        "# Load PIE dataset\n",
        "pie_df = pd.read_csv('pie_trajectory_sample.csv')\n",
        "print(pie_df.head())\n",
        "\n",
        "# Load combined dataset\n",
        "combined_df = pd.read_csv('combined_emotion_trajectory_sample.csv')\n",
        "print(combined_df.head())\n"
      ],
      "metadata": {
        "id": "FEagN_Amj01j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "z6dKYFGYO_DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse FER-2013 images\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for idx, row in fer2013_df.iterrows():\n",
        "    emotion = int(row['emotion'])\n",
        "    pixels = np.array([int(p) for p in row['pixels'].split()], dtype=np.float32) / 255.0\n",
        "    img = pixels.reshape(48, 48, 1)  # Reshape to 48x48 grayscale\n",
        "\n",
        "    X_train.append(img)\n",
        "    y_train.append(emotion)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "print(f\"Emotion data shape: {X_train.shape}\")  # (500, 48, 48, 1)\n",
        "\n",
        "# Parse PIE trajectories\n",
        "trajectories = []\n",
        "emotions = []\n",
        "\n",
        "for idx, row in pie_df.iterrows():\n",
        "    emotion = int(row['emotion'])\n",
        "    traj_frames = row['trajectory'].split('|')\n",
        "\n",
        "    traj_sequence = []\n",
        "    for frame_str in traj_frames:\n",
        "        x, y, w, h = map(float, frame_str.split('_'))\n",
        "        traj_sequence.append([x, y, w, h])\n",
        "\n",
        "    trajectories.append(np.array(traj_sequence))\n",
        "    emotions.append(emotion)\n",
        "\n",
        "trajectories = np.array(trajectories)\n",
        "emotions = np.array(emotions)\n",
        "print(f\"Trajectory data shape: {trajectories.shape}\")  # (500, 30, 4)\n"
      ],
      "metadata": {
        "id": "U_vGCsKAj6M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 1: Install Dependencies**"
      ],
      "metadata": {
        "id": "-Ql0hSlymQZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow keras scikit-learn numpy matplotlib opencv-python pillow pandas scipy\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import Rectangle\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# Set seeds\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"âœ… All dependencies installed!\")\n"
      ],
      "metadata": {
        "id": "ZiYIiqT9kARE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 2: Load and Parse Uploaded Datasets**"
      ],
      "metadata": {
        "id": "HX1jQG5XmjX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LOADING UPLOADED DATASETS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load the CSV files\n",
        "fer2013_df = pd.read_csv('fer2013_sample.csv')\n",
        "pie_df = pd.read_csv('pie_trajectory_sample.csv')\n",
        "combined_df = pd.read_csv('combined_emotion_trajectory_sample.csv')\n",
        "\n",
        "print(f\"âœ… FER-2013 loaded: {fer2013_df.shape}\")\n",
        "print(f\"âœ… PIE loaded: {pie_df.shape}\")\n",
        "print(f\"âœ… Combined loaded: {combined_df.shape}\")\n",
        "\n",
        "# Emotion labels\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n"
      ],
      "metadata": {
        "id": "fzuy4Nt9mfxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Section 3: Parse FER-2013 Emotion Data*"
      ],
      "metadata": {
        "id": "iMm9Zbspmp8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PARSING FER-2013 EMOTION DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "X_train_fer = []\n",
        "y_train_fer = []\n",
        "\n",
        "print(\"ðŸ”„ Parsing FER-2013 images...\")\n",
        "\n",
        "for idx, row in fer2013_df.iterrows():\n",
        "    emotion = int(row['emotion'])\n",
        "\n",
        "    # Parse pixel string into numpy array\n",
        "    pixels = np.array([int(p) for p in row['pixels'].split()], dtype=np.float32)\n",
        "\n",
        "    # Normalize to 0-1 range\n",
        "    pixels = pixels / 255.0\n",
        "\n",
        "    # Reshape to 48x48x1 (grayscale image)\n",
        "    img = pixels.reshape(48, 48, 1)\n",
        "\n",
        "    X_train_fer.append(img)\n",
        "    y_train_fer.append(emotion)\n",
        "\n",
        "X_train_fer = np.array(X_train_fer)\n",
        "y_train_fer = np.array(y_train_fer)\n",
        "\n",
        "print(f\"âœ… FER-2013 Images shape: {X_train_fer.shape}\")\n",
        "print(f\"   Emotions shape: {y_train_fer.shape}\")\n",
        "print(f\"\\nðŸ“Š Emotion distribution:\")\n",
        "for emotion_idx, label in enumerate(emotion_labels):\n",
        "    count = (y_train_fer == emotion_idx).sum()\n",
        "    print(f\"     {label:12} : {count:3d} samples\")\n",
        "\n",
        "# Split into train and test (80/20)\n",
        "X_fer_train, X_fer_test, y_fer_train, y_fer_test = train_test_split(\n",
        "    X_train_fer, y_train_fer, test_size=0.2, random_state=42, stratify=y_train_fer\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Train/Test split: {X_fer_train.shape[0]} train / {X_fer_test.shape[0]} test\")\n"
      ],
      "metadata": {
        "id": "ISuteRxMm5_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 4: Parse PIE Trajectory Data**"
      ],
      "metadata": {
        "id": "gSEgiKTtnFYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PARSING PIE TRAJECTORY DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "X_trajectories = []\n",
        "y_emotions_pie = []\n",
        "\n",
        "print(\"ðŸ”„ Parsing PIE trajectories...\")\n",
        "\n",
        "for idx, row in pie_df.iterrows():\n",
        "    emotion = int(row['emotion'])\n",
        "\n",
        "    # Parse trajectory frames (pipe-separated)\n",
        "    traj_frames = row['trajectory'].split('|')\n",
        "\n",
        "    # Parse each frame (x_y_w_h format)\n",
        "    trajectory_sequence = []\n",
        "    for frame_str in traj_frames:\n",
        "        x, y, w, h = map(float, frame_str.split('_'))\n",
        "        trajectory_sequence.append([x, y, w, h])\n",
        "\n",
        "    X_trajectories.append(np.array(trajectory_sequence, dtype=np.float32))\n",
        "    y_emotions_pie.append(emotion)\n",
        "\n",
        "X_trajectories = np.array(X_trajectories)\n",
        "y_emotions_pie = np.array(y_emotions_pie)\n",
        "\n",
        "print(f\"âœ… Trajectories shape: {X_trajectories.shape}\")\n",
        "print(f\"   Format: (sequences, frames, features) = (500, 30, 4)\")\n",
        "print(f\"âœ… Emotions shape: {y_emotions_pie.shape}\")\n",
        "\n",
        "# Normalize trajectories for LSTM\n",
        "X_trajectories_norm = (X_trajectories - X_trajectories.mean(axis=(0, 1))) / (X_trajectories.std(axis=(0, 1)) + 1e-6)\n",
        "\n",
        "print(f\"âœ… Trajectories normalized (mean=0, std=1)\")\n",
        "\n",
        "# Split into train and test (80/20)\n",
        "X_traj_train, X_traj_test, y_traj_train, y_traj_test = train_test_split(\n",
        "    X_trajectories_norm, y_emotions_pie, test_size=0.2, random_state=42, stratify=y_emotions_pie\n",
        ")\n",
        "\n",
        "print(f\"âœ… Trajectory Train/Test split: {X_traj_train.shape[0]} train / {X_traj_test.shape[0]} test\")\n"
      ],
      "metadata": {
        "id": "E4h1NjFmnCjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 5: Build and Train CNN Emotion Recognition Model**"
      ],
      "metadata": {
        "id": "f0sW3dNMnRWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BUILDING CNN EMOTION RECOGNITION MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def build_emotion_cnn_model(input_shape=(48, 48, 1), num_classes=7):\n",
        "    \"\"\"\n",
        "    Build CNN for emotion recognition (MobileNetV2-inspired architecture)\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        # Input layer\n",
        "        layers.Input(shape=input_shape),\n",
        "\n",
        "        # Block 1\n",
        "        layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Block 2: Depthwise Separable\n",
        "        layers.DepthwiseConv2D((3, 3), padding='same', activation='relu'),\n",
        "        layers.Conv2D(64, (1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Block 3\n",
        "        layers.DepthwiseConv2D((3, 3), padding='same', activation='relu'),\n",
        "        layers.Conv2D(128, (1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Block 4\n",
        "        layers.DepthwiseConv2D((3, 3), padding='same', activation='relu'),\n",
        "        layers.Conv2D(256, (1, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Global pooling\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "\n",
        "        # Dense layers\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "\n",
        "        # Output\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build emotion model\n",
        "emotion_model = build_emotion_cnn_model(input_shape=(48, 48, 1), num_classes=7)\n",
        "\n",
        "emotion_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"âœ… CNN Model built!\")\n",
        "print(emotion_model.summary())\n",
        "\n",
        "# Data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Train emotion model\n",
        "print(\"\\nðŸ”„ Training CNN Emotion Recognition Model...\")\n",
        "history_cnn = emotion_model.fit(\n",
        "    train_datagen.flow(X_fer_train, y_fer_train, batch_size=16),\n",
        "    epochs=15,\n",
        "    validation_data=(X_fer_test, y_fer_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "y_pred_fer = emotion_model.predict(X_fer_test, verbose=0).argmax(axis=1)\n",
        "emotion_accuracy = accuracy_score(y_fer_test, y_pred_fer)\n",
        "emotion_precision = precision_score(y_fer_test, y_pred_fer, average='weighted', zero_division=0)\n",
        "emotion_recall = recall_score(y_fer_test, y_pred_fer, average='weighted', zero_division=0)\n",
        "emotion_f1 = f1_score(y_fer_test, y_pred_fer, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"\\nâœ… CNN Emotion Model Performance:\")\n",
        "print(f\"   Accuracy:  {emotion_accuracy:.4f}\")\n",
        "print(f\"   Precision: {emotion_precision:.4f}\")\n",
        "print(f\"   Recall:    {emotion_recall:.4f}\")\n",
        "print(f\"   F1-Score:  {emotion_f1:.4f}\")"
      ],
      "metadata": {
        "id": "vkqx-a_FnN4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 6: Build and Train LSTM Trajectory Prediction Model**"
      ],
      "metadata": {
        "id": "NaOw8i4ipVEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BUILDING LSTM TRAJECTORY PREDICTION MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def build_lstm_trajectory_model(seq_length=30, num_features=4, num_classes=7):\n",
        "    \"\"\"\n",
        "    LSTM model for trajectory prediction with emotion auxiliary task\n",
        "    \"\"\"\n",
        "    trajectory_input = layers.Input(shape=(seq_length, num_features), name='trajectory_input')\n",
        "\n",
        "    # LSTM layers\n",
        "    x = layers.LSTM(128, return_sequences=True, activation='relu')(trajectory_input)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.LSTM(64, return_sequences=True, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.LSTM(32, return_sequences=False, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # Trajectory prediction head (predict next 5 frames)\n",
        "    trajectory_pred = layers.Dense(20, name='trajectory_prediction')(x)  # 5 frames * 4 features\n",
        "\n",
        "    # Emotion classification head (auxiliary task)\n",
        "    emotion_branch = layers.Dense(64, activation='relu')(x)\n",
        "    emotion_branch = layers.Dense(7, activation='softmax', name='emotion_prediction')(emotion_branch)\n",
        "\n",
        "    model = models.Model(\n",
        "        inputs=trajectory_input,\n",
        "        outputs=[trajectory_pred, emotion_branch]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build LSTM model\n",
        "lstm_model = build_lstm_trajectory_model(seq_length=30, num_features=4, num_classes=7)\n",
        "\n",
        "lstm_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss={\n",
        "        'trajectory_prediction': 'mse',\n",
        "        'emotion_prediction': 'sparse_categorical_crossentropy'\n",
        "    },\n",
        "    loss_weights={\n",
        "        'trajectory_prediction': 1.0,\n",
        "        'emotion_prediction': 0.5\n",
        "    },\n",
        "    metrics={\n",
        "        'trajectory_prediction': 'mae',\n",
        "        'emotion_prediction': 'accuracy'\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\" LSTM Model built!\")\n",
        "print(lstm_model.summary())\n",
        "\n",
        "# Ensure we have the correct 1D emotion labels for the train/test sets.\n",
        "# (Re-perform the split for the labels to guarantee correct shape, as\n",
        "# y_traj_train from a previous run appears to have an incorrect shape in the kernel state).\n",
        "# We use np.arange as dummy features to get the correct label splits.\n",
        "_, _, y_emotion_train_labels, y_emotion_test_labels = train_test_split(\n",
        "    np.arange(len(y_emotions_pie)),\n",
        "    y_emotions_pie,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_emotions_pie\n",
        ")\n",
        "\n",
        "# Prepare target trajectories (predict last 5 frames of input sequence)\n",
        "# Each sequence is 30 frames. Last 5 frames (5*4=20 features) as target.\n",
        "y_traj_prediction_train = X_traj_train[:, -5:, :].reshape(len(X_traj_train), -1)\n",
        "y_traj_prediction_test = X_traj_test[:, -5:, :].reshape(len(X_traj_test), -1)\n",
        "\n",
        "print(f\"\\n Target shapes:\")\n",
        "print(f\"   y_traj_prediction_train: {y_traj_prediction_train.shape}\")\n",
        "print(f\"   y_traj_prediction_test: {y_traj_prediction_test.shape}\")\n",
        "print(f\"   y_emotion_train_labels: {y_emotion_train_labels.shape}\")\n",
        "print(f\"   y_emotion_test_labels: {y_emotion_test_labels.shape}\")\n",
        "\n",
        "# Train LSTM\n",
        "print(\"\\n Training LSTM Trajectory Prediction Model...\")\n",
        "lstm_history = lstm_model.fit(\n",
        "    X_traj_train,\n",
        "    {\n",
        "        'trajectory_prediction': y_traj_prediction_train,\n",
        "        'emotion_prediction': y_emotion_train_labels # Use the explicitly defined labels\n",
        "    },\n",
        "    epochs=15,\n",
        "    batch_size=16,\n",
        "    validation_data=(\n",
        "        X_traj_test,\n",
        "        {\n",
        "            'trajectory_prediction': y_traj_prediction_test,\n",
        "            'emotion_prediction': y_emotion_test_labels # Use the explicitly defined labels\n",
        "        }\n",
        "    ),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\" LSTM Model training completed!\")"
      ],
      "metadata": {
        "id": "zWqLbf58pRm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Section 7: Emotion-to-Vehicle Behavior Mapping**"
      ],
      "metadata": {
        "id": "dSbJMSrrrdT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EMOTION-TO-VEHICLE BEHAVIOR MAPPING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class EmotionBehaviorMapper:\n",
        "    \"\"\"\n",
        "    Maps emotions to vehicle actions (BRAKE, CAUTION, PROCEED)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.emotion_to_action = {\n",
        "            0: {'action': 'BRAKE', 'speed_factor': 0.0, 'label': 'Angry'},\n",
        "            1: {'action': 'BRAKE', 'speed_factor': 0.0, 'label': 'Disgust'},\n",
        "            2: {'action': 'BRAKE', 'speed_factor': 0.0, 'label': 'Fear'},\n",
        "            3: {'action': 'PROCEED', 'speed_factor': 1.0, 'label': 'Happy'},\n",
        "            4: {'action': 'PROCEED', 'speed_factor': 1.0, 'label': 'Neutral'},\n",
        "            5: {'action': 'CAUTION', 'speed_factor': 0.6, 'label': 'Sad'},\n",
        "            6: {'action': 'CAUTION', 'speed_factor': 0.6, 'label': 'Surprise'}\n",
        "        }\n",
        "\n",
        "    def get_vehicle_action(self, emotion_idx, confidence=1.0):\n",
        "        \"\"\"Get vehicle action based on emotion\"\"\"\n",
        "        action_info = self.emotion_to_action[emotion_idx]\n",
        "\n",
        "        if confidence < 0.7:\n",
        "            return {\n",
        "                'action': 'CAUTION',\n",
        "                'speed_factor': 0.6,\n",
        "                'confidence': confidence,\n",
        "                'label': f\"{action_info['label']} (LOW CONF)\"\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'action': action_info['action'],\n",
        "            'speed_factor': action_info['speed_factor'],\n",
        "            'confidence': confidence,\n",
        "            'label': action_info['label']\n",
        "        }\n",
        "\n",
        "    def adjust_trajectory(self, emotion_idx, current_trajectory, base_speed=50):\n",
        "        \"\"\"Adjust vehicle speed and path based on emotion\"\"\"\n",
        "        action_info = self.get_vehicle_action(emotion_idx)\n",
        "\n",
        "        new_speed = base_speed * action_info['speed_factor']\n",
        "\n",
        "        if action_info['action'] == 'CAUTION':\n",
        "            path_adjustment = 2.0\n",
        "        elif action_info['action'] == 'BRAKE':\n",
        "            path_adjustment = 0.0\n",
        "        else:\n",
        "            path_adjustment = 0.0\n",
        "\n",
        "        return {\n",
        "            'new_speed': new_speed,\n",
        "            'path_adjustment': path_adjustment,\n",
        "            'vehicle_action': action_info['action'],\n",
        "            'emotion_label': action_info['label']\n",
        "        }\n",
        "\n",
        "emotion_mapper = EmotionBehaviorMapper()\n",
        "\n",
        "print(\"\\nðŸ“Š Emotion-to-Action Mapping Table:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Emotion':<15} {'Action':<10} {'Speed Factor':<12} {'Description':<35}\")\n",
        "print(\"-\" * 70)\n",
        "for emotion_idx, action_info in emotion_mapper.emotion_to_action.items():\n",
        "    print(f\"{action_info['label']:<15} {action_info['action']:<10} {action_info['speed_factor']:<12.2f}\")\n",
        "print(\"-\" * 70)\n"
      ],
      "metadata": {
        "id": "vv9Mr9OprRLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 8: Quantum-Inspired Probabilistic Decision Engine**"
      ],
      "metadata": {
        "id": "BMTHsJ8krlFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"QUANTUM-INSPIRED PROBABILISTIC DECISION ENGINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class QuantumEmotionDecisionEngine:\n",
        "    \"\"\"\n",
        "    Quantum-inspired logic for handling ambiguous emotional states.\n",
        "    Uses probabilistic superposition and amplitude-based measurement.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_emotions=7):\n",
        "        self.num_emotions = num_emotions\n",
        "        self.action_space = ['BRAKE', 'CAUTION', 'PROCEED']\n",
        "        self.emotion_action_map = {\n",
        "            'BRAKE': [0, 1, 2],           # Angry, Disgust, Fear\n",
        "            'CAUTION': [5, 6],            # Sad, Surprise\n",
        "            'PROCEED': [3, 4]             # Happy, Neutral\n",
        "        }\n",
        "\n",
        "    def create_superposition_state(self, emotion_probabilities):\n",
        "        \"\"\"Create quantum superposition of emotional states\"\"\"\n",
        "        amplitudes = np.sqrt(emotion_probabilities)\n",
        "        amplitudes = amplitudes / np.linalg.norm(amplitudes)\n",
        "        return amplitudes\n",
        "\n",
        "    def compute_action_probabilities(self, emotion_probabilities, confidence):\n",
        "        \"\"\"Compute action probability using quantum logic\"\"\"\n",
        "        amplitudes = self.create_superposition_state(emotion_probabilities)\n",
        "        measurement_probs = amplitudes ** 2\n",
        "\n",
        "        action_probs = {action: 0.0 for action in self.action_space}\n",
        "\n",
        "        for emotion_idx, emotion_amp in enumerate(measurement_probs):\n",
        "            for action, emotion_list in self.emotion_action_map.items():\n",
        "                if emotion_idx in emotion_list:\n",
        "                    action_probs[action] += emotion_amp\n",
        "\n",
        "        total = sum(action_probs.values())\n",
        "        if total > 0:\n",
        "            action_probs = {k: v/total for k, v in action_probs.items()}\n",
        "\n",
        "        # Confidence modifier\n",
        "        if confidence > 0.8:\n",
        "            max_action = max(action_probs, key=action_probs.get)\n",
        "            action_probs = {k: 0.0 for k in action_probs.keys()}\n",
        "            action_probs[max_action] = 1.0\n",
        "        elif confidence < 0.5:\n",
        "            uniform = {k: 1.0/len(action_probs) for k in action_probs.keys()}\n",
        "            action_probs = {k: 0.5*action_probs[k] + 0.5*uniform[k] for k in action_probs.keys()}\n",
        "\n",
        "        return action_probs\n",
        "\n",
        "    def make_decision(self, emotion_probabilities, confidence_threshold=0.5):\n",
        "        \"\"\"Make final vehicle action decision\"\"\"\n",
        "        max_confidence = np.max(emotion_probabilities)\n",
        "        action_probs = self.compute_action_probabilities(emotion_probabilities, max_confidence)\n",
        "        selected_action = max(action_probs, key=action_probs.get)\n",
        "\n",
        "        return {\n",
        "            'selected_action': selected_action,\n",
        "            'action_probabilities': action_probs,\n",
        "            'confidence': max_confidence,\n",
        "            'is_uncertain': max_confidence < confidence_threshold\n",
        "        }\n",
        "\n",
        "quantum_engine = QuantumEmotionDecisionEngine(num_emotions=7)\n",
        "\n",
        "print(\"âœ… Quantum-inspired decision engine initialized!\")\n",
        "\n",
        "# Demo quantum logic\n",
        "print(\"\\nðŸ”¬ Quantum Decision Logic Examples:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Example 1: Clear emotion\n",
        "test_probs_clear = np.array([0.01, 0.01, 0.01, 0.95, 0.01, 0.01, 0.00])\n",
        "decision_clear = quantum_engine.make_decision(test_probs_clear)\n",
        "print(f\"Clear Emotion (Happy, 95% confidence):\")\n",
        "print(f\"  Selected Action: {decision_clear['selected_action']}\")\n",
        "print(f\"  Action Probabilities: {decision_clear['action_probabilities']}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Example 2: Ambiguous emotion\n",
        "test_probs_ambiguous = np.array([0.20, 0.15, 0.25, 0.15, 0.15, 0.05, 0.05])\n",
        "decision_ambiguous = quantum_engine.make_decision(test_probs_ambiguous)\n",
        "print(f\"Ambiguous Emotion (Mixed signals, ~40% max confidence):\")\n",
        "print(f\"  Selected Action: {decision_ambiguous['selected_action']}\")\n",
        "print(f\"  Action Probabilities: {decision_ambiguous['action_probabilities']}\")\n",
        "print(f\"  Is Uncertain: {decision_ambiguous['is_uncertain']}\")"
      ],
      "metadata": {
        "id": "1QVVsfY5rkgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 9: Real-Time Prediction Pipeline**"
      ],
      "metadata": {
        "id": "RIVx1fg1rvvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"REAL-TIME PREDICTION PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class RealTimePredictionPipeline:\n",
        "    \"\"\"\n",
        "    Complete pipeline for AV perception and decision-making\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emotion_model, lstm_model, emotion_mapper, quantum_engine):\n",
        "        self.emotion_model = emotion_model\n",
        "        self.lstm_model = lstm_model\n",
        "        self.emotion_mapper = emotion_mapper\n",
        "        self.quantum_engine = quantum_engine\n",
        "\n",
        "        self.frame_count = 0\n",
        "        self.processing_times = []\n",
        "        self.decisions_history = []\n",
        "\n",
        "    def predict_frame(self, face_image, trajectory_sequence, use_quantum=True):\n",
        "        \"\"\"Process single frame and make vehicle decision\"\"\"\n",
        "        import time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Step 1: Emotion recognition\n",
        "        face_batch = np.expand_dims(face_image, axis=0)\n",
        "        emotion_probs = self.emotion_model.predict(face_batch, verbose=0)[0]\n",
        "\n",
        "        # Step 2: Trajectory prediction\n",
        "        traj_batch = np.expand_dims(trajectory_sequence, axis=0)\n",
        "        next_trajectory, emotion_aux = self.lstm_model.predict(traj_batch, verbose=0)\n",
        "\n",
        "        # Step 3: Decide vehicle action\n",
        "        if use_quantum:\n",
        "            decision_result = self.quantum_engine.make_decision(emotion_probs)\n",
        "            selected_emotion = np.argmax(emotion_probs)\n",
        "            vehicle_action = decision_result['selected_action']\n",
        "        else:\n",
        "            selected_emotion = np.argmax(emotion_probs)\n",
        "            action_info = self.emotion_mapper.get_vehicle_action(selected_emotion, emotion_probs[selected_emotion])\n",
        "            vehicle_action = action_info['action']\n",
        "\n",
        "        # Step 4: Adjust trajectory\n",
        "        adjustment = self.emotion_mapper.adjust_trajectory(selected_emotion, trajectory_sequence, base_speed=50)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "        self.processing_times.append(processing_time)\n",
        "        self.frame_count += 1\n",
        "\n",
        "        result = {\n",
        "            'frame_number': self.frame_count,\n",
        "            'emotion_probabilities': emotion_probs,\n",
        "            'detected_emotion': selected_emotion,\n",
        "            'emotion_label': emotion_labels[selected_emotion],\n",
        "            'confidence': emotion_probs[selected_emotion],\n",
        "            'predicted_trajectory': next_trajectory,\n",
        "            'vehicle_action': vehicle_action,\n",
        "            'speed_adjustment': adjustment['new_speed'],\n",
        "            'path_adjustment': adjustment['path_adjustment'],\n",
        "            'processing_time_ms': processing_time * 1000,\n",
        "            'use_quantum': use_quantum\n",
        "        }\n",
        "\n",
        "        self.decisions_history.append(result)\n",
        "        return result\n",
        "\n",
        "    def get_statistics(self):\n",
        "        \"\"\"Return processing statistics\"\"\"\n",
        "        times_ms = np.array(self.processing_times) * 1000\n",
        "        return {\n",
        "            'total_frames_processed': self.frame_count,\n",
        "            'avg_processing_time_ms': np.mean(times_ms),\n",
        "            'max_processing_time_ms': np.max(times_ms),\n",
        "            'min_processing_time_ms': np.min(times_ms),\n",
        "            'real_time_capability': np.mean(times_ms) < 100\n",
        "        }\n",
        "\n",
        "# Initialize pipeline\n",
        "pipeline = RealTimePredictionPipeline(\n",
        "    emotion_model=emotion_model,\n",
        "    lstm_model=lstm_model,\n",
        "    emotion_mapper=emotion_mapper,\n",
        "    quantum_engine=quantum_engine\n",
        ")\n",
        "\n",
        "print(\"âœ… Real-time prediction pipeline initialized!\")\n"
      ],
      "metadata": {
        "id": "wWwosgAdrvIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 10: Simulate Frame-by-Frame Processing**"
      ],
      "metadata": {
        "id": "yCDB51ffr7Sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SIMULATING REAL-TIME FRAME-BY-FRAME PROCESSING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "num_test_frames = 15\n",
        "test_results = []\n",
        "\n",
        "print(f\"\\nðŸŽ¬ Processing {num_test_frames} frames...\")\n",
        "\n",
        "for frame_idx in range(num_test_frames):\n",
        "    # Random test face and trajectory\n",
        "    test_face = X_fer_test[np.random.randint(0, len(X_fer_test))]\n",
        "    test_traj = X_traj_test[np.random.randint(0, len(X_traj_test))]\n",
        "\n",
        "    # Process frame\n",
        "    result = pipeline.predict_frame(test_face, test_traj, use_quantum=True)\n",
        "    test_results.append(result)\n",
        "\n",
        "    if (frame_idx + 1) % 5 == 0:\n",
        "        print(f\"  âœ“ Processed {frame_idx + 1} frames...\")\n",
        "\n",
        "print(f\"\\nâœ… Completed processing {len(test_results)} frames\")\n",
        "\n",
        "# Display sample results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Sample Frame Processing Results:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i in [0, len(test_results)//2, -1]:\n",
        "    r = test_results[i]\n",
        "    print(f\"\\nFrame {r['frame_number']}:\")\n",
        "    print(f\"  Detected Emotion: {r['emotion_label']} (confidence: {r['confidence']:.3f})\")\n",
        "    print(f\"  Vehicle Action: {r['vehicle_action']}\")\n",
        "    print(f\"  Speed Adjustment: {r['speed_adjustment']:.1f} km/h\")\n",
        "    print(f\"  Processing Time: {r['processing_time_ms']:.2f} ms\")"
      ],
      "metadata": {
        "id": "i6uG0tgRr44M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 11: Evaluation Metrics**"
      ],
      "metadata": {
        "id": "XgkU52ZtsKUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION METRICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "emotions_predicted = [r['detected_emotion'] for r in test_results]\n",
        "confidences = [r['confidence'] for r in test_results]\n",
        "vehicle_actions = [r['vehicle_action'] for r in test_results]\n",
        "\n",
        "stats = pipeline.get_statistics()\n",
        "\n",
        "print(f\"\\nðŸ“Š Processing Statistics:\")\n",
        "print(f\"  Total Frames Processed: {stats['total_frames_processed']}\")\n",
        "print(f\"  Avg Processing Time: {stats['avg_processing_time_ms']:.2f} ms\")\n",
        "print(f\"  Max Processing Time: {stats['max_processing_time_ms']:.2f} ms\")\n",
        "print(f\"  Min Processing Time: {stats['min_processing_time_ms']:.2f} ms\")\n",
        "print(f\"  Real-time Capability: {'âœ… YES' if stats['real_time_capability'] else 'âŒ NO'} ({stats['avg_processing_time_ms']:.2f} ms < 100 ms required)\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Emotion Recognition Statistics:\")\n",
        "print(f\"  Average Confidence: {np.mean(confidences):.4f}\")\n",
        "print(f\"  Confidence Std Dev: {np.std(confidences):.4f}\")\n",
        "print(f\"  High Confidence (>0.7): {sum(1 for c in confidences if c > 0.7)} frames\")\n",
        "print(f\"  Medium Confidence (0.5-0.7): {sum(1 for c in confidences if 0.5 <= c <= 0.7)} frames\")\n",
        "print(f\"  Low Confidence (<0.5): {sum(1 for c in confidences if c < 0.5)} frames\")\n",
        "\n",
        "action_counts = pd.Series(vehicle_actions).value_counts()\n",
        "print(f\"\\nðŸš— Vehicle Action Distribution:\")\n",
        "for action, count in action_counts.items():\n",
        "    print(f\"  {action}: {count} frames ({100*count/len(vehicle_actions):.1f}%)\")\n"
      ],
      "metadata": {
        "id": "0pMr3JlxsE9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 12: Visualization**"
      ],
      "metadata": {
        "id": "ulT2dkPNsQF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING VISUALIZATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Emotion Model Training Accuracy\n",
        "ax1 = plt.subplot(3, 3, 1)\n",
        "ax1.plot(history_cnn.history['accuracy'], label='Train Accuracy')\n",
        "ax1.plot(history_cnn.history['val_accuracy'], label='Validation Accuracy')\n",
        "ax1.set_title('CNN - Training Accuracy', fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Plot 2: Emotion Model Training Loss\n",
        "ax2 = plt.subplot(3, 3, 2)\n",
        "ax2.plot(history_cnn.history['loss'], label='Train Loss')\n",
        "ax2.plot(history_cnn.history['val_loss'], label='Validation Loss')\n",
        "ax2.set_title('CNN - Training Loss', fontsize=12, fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# Plot 3: Confusion Matrix\n",
        "ax3 = plt.subplot(3, 3, 3)\n",
        "cm = confusion_matrix(y_fer_test, y_pred_fer)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax3, cbar=False,\n",
        "            xticklabels=emotion_labels, yticklabels=emotion_labels)\n",
        "ax3.set_title('Emotion Classification - Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "ax3.set_ylabel('True')\n",
        "ax3.set_xlabel('Predicted')\n",
        "\n",
        "# Plot 4: LSTM Trajectory Loss\n",
        "ax4 = plt.subplot(3, 3, 4)\n",
        "ax4.plot(lstm_history.history['trajectory_prediction_loss'], label='Trajectory Loss')\n",
        "ax4.plot(lstm_history.history['val_trajectory_prediction_loss'], label='Val Trajectory Loss')\n",
        "ax4.set_title('LSTM - Trajectory Prediction Loss', fontsize=12, fontweight='bold')\n",
        "ax4.set_xlabel('Epoch')\n",
        "ax4.set_ylabel('Loss (MSE)')\n",
        "ax4.legend()\n",
        "ax4.grid(alpha=0.3)\n",
        "\n",
        "# Plot 5: LSTM Emotion Loss\n",
        "ax5 = plt.subplot(3, 3, 5)\n",
        "ax5.plot(lstm_history.history['emotion_prediction_loss'], label='Emotion Loss')\n",
        "ax5.plot(lstm_history.history['val_emotion_prediction_loss'], label='Val Emotion Loss')\n",
        "ax5.set_title('LSTM - Emotion Auxiliary Loss', fontsize=12, fontweight='bold')\n",
        "ax5.set_xlabel('Epoch')\n",
        "ax5.set_ylabel('Loss')\n",
        "ax5.legend()\n",
        "ax5.grid(alpha=0.3)\n",
        "\n",
        "# Plot 6: Frame Processing Time\n",
        "ax6 = plt.subplot(3, 3, 6)\n",
        "processing_times_ms = [r['processing_time_ms'] for r in test_results]\n",
        "ax6.plot(processing_times_ms, marker='o', linestyle='-', linewidth=2, markersize=6)\n",
        "ax6.axhline(y=100, color='r', linestyle='--', label='Real-time Threshold (100ms)')\n",
        "ax6.set_title('Frame Processing Time Over Time', fontsize=12, fontweight='bold')\n",
        "ax6.set_xlabel('Frame Number')\n",
        "ax6.set_ylabel('Processing Time (ms)')\n",
        "ax6.legend()\n",
        "ax6.grid(alpha=0.3)\n",
        "\n",
        "# Plot 7: Emotion Distribution\n",
        "ax7 = plt.subplot(3, 3, 7)\n",
        "emotion_counts = pd.Series([r['emotion_label'] for r in test_results]).value_counts()\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(emotion_counts)))\n",
        "ax7.barh(emotion_counts.index, emotion_counts.values, color=colors)\n",
        "ax7.set_title('Detected Emotions Distribution', fontsize=12, fontweight='bold')\n",
        "ax7.set_xlabel('Count')\n",
        "\n",
        "# Plot 8: Confidence Distribution\n",
        "ax8 = plt.subplot(3, 3, 8)\n",
        "ax8.hist(confidences, bins=15, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "ax8.axvline(x=np.mean(confidences), color='r', linestyle='--', label=f'Mean: {np.mean(confidences):.3f}')\n",
        "ax8.set_title('Emotion Confidence Distribution', fontsize=12, fontweight='bold')\n",
        "ax8.set_xlabel('Confidence')\n",
        "ax8.set_ylabel('Frequency')\n",
        "ax8.legend()\n",
        "ax8.grid(alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 9: Vehicle Actions Pie Chart\n",
        "ax9 = plt.subplot(3, 3, 9)\n",
        "action_counts = pd.Series(vehicle_actions).value_counts()\n",
        "colors_actions = ['#ff6b6b', '#ffd93d', '#6bcf7f']\n",
        "ax9.pie(action_counts.values, labels=action_counts.index, autopct='%1.1f%%',\n",
        "        colors=colors_actions[:len(action_counts)], startangle=90)\n",
        "ax9.set_title('Vehicle Action Distribution', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('emotion_trajectory_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Main visualization saved as 'emotion_trajectory_analysis.png'\")"
      ],
      "metadata": {
        "id": "sLJtwRyCsPc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 13: Video Frame Visualization with Overlays**"
      ],
      "metadata": {
        "id": "xWMnW7_GscpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VIDEO FRAME VISUALIZATION WITH OVERLAYS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "fig.suptitle('Real-time AV Perception: Emotion & Trajectory Visualization',\n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "sample_indices = [0, 3, 6, 9, 12, 14] if len(test_results) >= 15 else range(len(test_results))\n",
        "\n",
        "for idx, sample_idx in enumerate(sample_indices):\n",
        "    ax = axes[idx // 3, idx % 3]\n",
        "\n",
        "    if sample_idx < len(test_results):\n",
        "        result = test_results[sample_idx]\n",
        "\n",
        "        # Create mock video frame\n",
        "        frame = np.ones((480, 640, 3), dtype=np.uint8) * 200\n",
        "\n",
        "        # Pedestrian bounding box\n",
        "        ped_x, ped_y, ped_w, ped_h = 300, 150, 80, 200\n",
        "        cv2.rectangle(frame, (ped_x - ped_w//2, ped_y - ped_h//2),\n",
        "                      (ped_x + ped_w//2, ped_y + ped_h//2), (0, 255, 0), 3)\n",
        "\n",
        "        # Emotion label with color\n",
        "        emotion_color = {\n",
        "            'Happy': (0, 255, 0),\n",
        "            'Neutral': (255, 255, 0),\n",
        "            'Sad': (0, 0, 255),\n",
        "            'Angry': (0, 0, 128),\n",
        "            'Disgust': (128, 0, 128),\n",
        "            'Fear': (255, 0, 255),\n",
        "            'Surprise': (0, 255, 255)\n",
        "        }\n",
        "\n",
        "        color = emotion_color.get(result['emotion_label'], (255, 255, 255))\n",
        "        cv2.putText(frame, f\"Emotion: {result['emotion_label']}\", (20, 50),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 2)\n",
        "        cv2.putText(frame, f\"Conf: {result['confidence']:.3f}\", (20, 100),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
        "\n",
        "        # Vehicle action\n",
        "        action_color = {\n",
        "            'BRAKE': (0, 0, 255),\n",
        "            'CAUTION': (0, 165, 255),\n",
        "            'PROCEED': (0, 255, 0)\n",
        "        }\n",
        "        action_col = action_color.get(result['vehicle_action'], (255, 255, 255))\n",
        "        cv2.putText(frame, f\"Action: {result['vehicle_action']}\", (20, 400),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, action_col, 2)\n",
        "        cv2.putText(frame, f\"Speed: {result['speed_adjustment']:.1f} km/h\", (20, 450),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
        "\n",
        "        # Predicted trajectory arrow\n",
        "        cv2.arrowedLine(frame, (ped_x, ped_y), (ped_x + 80, ped_y + 50),\n",
        "                        (255, 0, 0), 3, tipLength=0.3)\n",
        "        cv2.putText(frame, \"Predicted Path\", (ped_x + 20, ped_y - 40),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
        "\n",
        "        cv2.putText(frame, f\"Frame {result['frame_number']}\", (20, 150),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
        "\n",
        "        ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    ax.set_title(f\"Frame {sample_idx + 1}\", fontsize=11, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('av_perception_frames.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Frame visualization saved as 'av_perception_frames.png'\")\n"
      ],
      "metadata": {
        "id": "sxSxqScTsccW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 14: Results Summary**"
      ],
      "metadata": {
        "id": "PMLl8vMWsqcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\"\"\n",
        "{'='*70}\n",
        "QUANTUM-ENHANCED EMOTION-AWARE AUTONOMOUS VEHICLE SYSTEM\n",
        "COMPLETE PERFORMANCE REPORT\n",
        "{'='*70}\n",
        "\n",
        "ðŸ“Š EMOTION RECOGNITION MODEL (CNN)\n",
        "{'â”€'*70}\n",
        "  Overall Accuracy:  {emotion_accuracy:.4f}\n",
        "  Weighted Precision: {emotion_precision:.4f}\n",
        "  Weighted Recall:    {emotion_recall:.4f}\n",
        "  Weighted F1-Score:  {emotion_f1:.4f}\n",
        "\n",
        "ðŸ“Š LSTM TRAJECTORY PREDICTION MODEL\n",
        "{'â”€'*70}\n",
        "  Input Sequence Length: 30 frames\n",
        "  Prediction Horizon: 5 frames\n",
        "  Features per Frame: 4 (x, y, w, h)\n",
        "\n",
        "â±ï¸ REAL-TIME PROCESSING STATISTICS\n",
        "{'â”€'*70}\n",
        "  Total Frames Processed: {stats['total_frames_processed']}\n",
        "  Average Processing Time: {stats['avg_processing_time_ms']:.2f} ms\n",
        "  Max Processing Time: {stats['max_processing_time_ms']:.2f} ms\n",
        "  Min Processing Time: {stats['min_processing_time_ms']:.2f} ms\n",
        "  Real-time Capability: {'âœ… PASS' if stats['real_time_capability'] else 'âŒ FAIL'} (Threshold: <100ms)\n",
        "  Frames Per Second: {1000 / stats['avg_processing_time_ms']:.1f} FPS\n",
        "\n",
        "ðŸ§  QUANTUM-INSPIRED DECISION ENGINE\n",
        "{'â”€'*70}\n",
        "  Decision Logic: Probabilistic superposition with amplitude-based measurement\n",
        "  Ambiguous Cases Handled: {sum(1 for r in test_results if r['confidence'] < 0.7)} frames\n",
        "  Average Confidence: {np.mean(confidences):.4f}\n",
        "  High Confidence (>0.7): {sum(1 for c in confidences if c > 0.7)} frames\n",
        "  Medium Confidence (0.5-0.7): {sum(1 for c in confidences if 0.5 <= c <= 0.7)} frames\n",
        "  Low Confidence (<0.5): {sum(1 for c in confidences if c < 0.5)} frames\n",
        "\n",
        "ðŸš— VEHICLE ACTION RESPONSE\n",
        "{'â”€'*70}\n",
        "\"\"\")\n",
        "\n",
        "for action, count in action_counts.items():\n",
        "    print(f\"  {action:8} | {count:2d} frames ({100*count/len(vehicle_actions):5.1f}%)\")\n",
        "\n",
        "print(f\"\"\"\n",
        "ðŸ“ˆ EMOTION-ACTION MAPPING\n",
        "{'â”€'*70}\n",
        "  Angry      â†’ BRAKE        (Stop immediately - high risk)\n",
        "  Disgust    â†’ BRAKE        (Stop immediately - high risk)\n",
        "  Fear       â†’ BRAKE        (Stop immediately - high risk)\n",
        "  Happy      â†’ PROCEED      (Safe pedestrian - continue)\n",
        "  Neutral    â†’ PROCEED      (Safe pedestrian - continue)\n",
        "  Sad        â†’ CAUTION      (Reduced speed - 60% speed)\n",
        "  Surprise   â†’ CAUTION      (Reduced speed - 60% speed)\n",
        "\n",
        "âœ… DATASETS USED\n",
        "{'â”€'*70}\n",
        "  FER-2013:\n",
        "    - Samples used: {len(fer2013_df)}\n",
        "    - Train/Test: {len(X_fer_train)} / {len(X_fer_test)}\n",
        "    - Image size: 48Ã—48 grayscale\n",
        "    - Emotions: 7 classes\n",
        "\n",
        "  PIE (Pedestrian Trajectory):\n",
        "    - Sequences: {len(pie_df)}\n",
        "    - Frames per sequence: 30\n",
        "    - Features: x, y, width, height\n",
        "    - Emotions: 7 classes\n",
        "\n",
        "ðŸ”¬ QUANTUM THEORY PRINCIPLES APPLIED\n",
        "{'â”€'*70}\n",
        "  âœ“ Superposition: Emotional states as probabilistic amplitudes\n",
        "  âœ“ Measurement: |amplitude|Â² determines final action probability\n",
        "  âœ“ Uncertainty: Low confidence â†’ increased superposition (softer decisions)\n",
        "  âœ“ Real-world Validation: Handles ambiguous pedestrian behaviors\n",
        "\n",
        "ðŸ“ CONCLUSION\n",
        "{'â”€'*70}\n",
        "The implemented system successfully demonstrates:\n",
        "  âœ“ Real-time facial emotion recognition (CNN accuracy: {emotion_accuracy:.1%})\n",
        "  âœ“ LSTM-based spatio-temporal trajectory prediction\n",
        "  âœ“ Quantum-inspired probabilistic decision logic\n",
        "  âœ“ Adaptive vehicle response (speed & trajectory adjustment)\n",
        "  âœ“ Real-time processing capability ({stats['avg_processing_time_ms']:.1f} ms per frame)\n",
        "\n",
        "This system can improve pedestrian safety in autonomous vehicles by:\n",
        "  â€¢ Detecting pedestrian emotional states from facial expressions\n",
        "  â€¢ Predicting pedestrian trajectory patterns\n",
        "  â€¢ Making robust decisions even with ambiguous signals\n",
        "  â€¢ Adapting vehicle behavior for pedestrian safety\n",
        "\n",
        "{'='*70}\n",
        "\"\"\")\n",
        "\n",
        "print(\"âœ… ANALYSIS COMPLETE!\")\n",
        "print(f\"Generated visualizations:\")\n",
        "print(f\"  - emotion_trajectory_analysis.png\")\n",
        "print(f\"  - av_perception_frames.png\")"
      ],
      "metadata": {
        "id": "dc2-wa8osptT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}